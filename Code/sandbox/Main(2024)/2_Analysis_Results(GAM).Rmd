
---
title: "2.Analysis_Results(GAM)"
author: "Jaeseok Hwang"
date: "`r Sys.Date()`"
output:
  html_document: default
  pdf_document: default
  word_document: defaulta
---




## Knitr option

```{r, cache = F, echo = F, results = "hide"}
#####

library(knitr)

knitr::opts_chunk$set(
  cache = FALSE,
  echo = FALSE,
  warning = FALSE,
  cache.lazy = FALSE,
  fig.retina = 6,
  fig.height = 9,
  fig.width = 9,
  message = FALSE,
  error = TRUE
)

options(knitr.duplicate.label = "allow")

```


#### Packages 

```{r pacakages, cache = FALSE, results = "hide"}

library(here)
library(rmarkdown) # for rmarkdown options
library(jsonlite) # for json data loading and processing
library(parallel) # for parallel processing (computing)
library(bookdown) # for bookdown options
library(knitr) # for knitr options
library(stringr) # for string manipulation
library(measurements) # for unit conversion
library(data.table) # for data manipulation
library(tidyverse) # for data manipulation
library(dplyr) # for data manipulation
library(tmap) # for mapping
library(ggplot2) # for plotting
library(sf) # for spatial data
library(mgcv) # for GAM
library(smoother) # for GAM
library(quantreg) # for quantile reg

###################



```

# Read sources and load processed data

```{r source, echo = F, results = "hide"}

# Read functions for data processing 
source(here("Code","Main","0_Set_up_preparation.R"))
source(here("Code","Functions","functions_for_analysis.R"))


ffy_merged_dat <- list.files(here("Data","Processed","Analysis_ready")) %>%
 str_subset("_merged_data.rds") %>%
   str_remove("_merged_data.rds")


ffy_weather_info <- list.files(here("Data","Processed","Analysis_ready")) %>%
 str_subset("_weather_info.rds") %>%
   str_remove("_weather_info.rds")

match(ffy_merged_dat, ffy_weather_info)




```



```{r analysis GAM, cache = T, results = "hide"}
 
### Check all the field_year list (ffy) in the processed data(Analysis_Ready) folder

# Run and evaluate GAMs with all the considered formula
# for each experimental trial field

info_tb_list <- list()
best_gam_list <- list()

for(i in 1:length(ffy_merged_dat)) {
  
 ffy_id <- ffy_merged_dat[i]
  # Read Sf data and weather info table
  dat_sf <- readRDS(here("Data", "processed", "Analysis_ready", paste0(ffy_id, "_merged_data.rds")))
  
  dat_sf <- dat_sf %>% st_transform(4326)
 # Drop geometry and convert to data.table
  dat_tb <- dat_sf %>% st_drop_geometry() %>% as.data.table()
  
  # Get input variables containing "rate"
  input_vars <- colnames(dat_tb)[grep("rate", colnames(dat_tb))]
  
  # set field regression variables for making evaluation data
  except_s <- setdiff(colnames(dat_tb), c("s_rate", "yield", 'obs_id'))
 
# Normalize the columns
# dat_tb[, ( except_s) := lapply(.SD, function(x) (x - mean(x, na.rm = TRUE)) / sd(x, na.rm = TRUE)), .SDcols = except_s]

  # Generate all possible GAM formulas
  gam_formulas <- generate_gam_formulas(input_vars, field_reg_vars)

  # Run and evaluate GAM models
  best_formula <- run_and_evaluate_gams(gam_formulas, dat_tb)
  
  # Update info_tb with best formulas using data.table syntax

  ### predict yield values at input sequence 
   # Generate input rate sequence (100 points) 
   input_s_seq  <-  dat_tb[, .(
      s_rate = seq(
        quantile(s_rate, 0.025),
        quantile(s_rate, 0.975),
        length = 100
      )
      )]

  # generate evaluation data to predict yield values by model 
  dat_for_eval <- input_s_seq[, (except_s) := lapply(dat_tb[, .SD, .SDcols = except_s], 
    function(x) mean(x, na.rm = TRUE))]


 # predict yield by reml and gcv method based gam regression model, respectively
   # predict_yield_range(data_for_evaluation, formula)# 
 
  eval_dat  <-  predict_yield_range(best_formula$gam_best, dat_for_eval)

  # Predict and evaluate estimated profit under the given crop price and input price
  # of the trial year, year that the seed/corn price ratio is lowest (low) , 
  # and year that the seed/corn price ratio is highest (high)

  info_tb <- readRDS(here("Data", "processed", "Analysis_ready", paste0(ffy_id, "_weather_info.rds")))
   
  setnames(info_tb, "gc_rate", "sqsr")
   
   info_tb <- info_tb %>% mutate(sqsr = ifelse(sqsr >100, sqsr/1000, sqsr))

  info_tb$year <- as.numeric(str_extract(info_tb$ffy_id, "\\d{4}"))
  
  #  estimate_profit_fcn(info_table, evaluation_data, price_table)
  eval_tb  <- estimate_profit_fcn(info_tb, eval_dat, price_tab) 


  # Estimates and report  eosr and profit by eosr
  # Estimates the profit by sqsr and usdasr
   

  # Track centroid of the ofpe field 
  cent_dat <- dat_sf %>%
  st_bbox() %>%
  st_as_sfc() %>%
  st_centroid() %>%
  st_as_sf() 

   # find which state the field is located in
   cent_in_ofpe <- st_join(cent_dat, ofpe_sf, join = st_within)

  # mutate usdsr variable by taking matched fips code.
     info_tb$usdsr <- seed_usda$s_rate[seed_usda$year == info_tb$year & seed_usda$fips == cent_in_ofpe$fips]

  # mutate seeding rate info, yield and profit by seeding rate
   info_tb[, `:=`( 
   avg_yield = mean(dat_sf$yield),
   s_min = eval_tb[which.min(s_rate), s_rate],
   s_max = eval_tb[which.max(s_rate), s_rate], 
   eosr = eval_tb[which.max(profit_hat_year), s_rate],
   eosr_low = eval_tb[which.max(profit_hat_low), s_rate],
   eosr_high = eval_tb[which.max(profit_hat_high), s_rate],
   eosr_y = eval_tb[which.max(profit_hat_year), yield_hat],
   eosr_y_low = eval_tb[which.max(profit_hat_low), yield_hat],
   eosr_y_high = eval_tb[which.max(profit_hat_high), yield_hat],
   eosr_p = eval_tb[which.max(profit_hat_year), profit_hat_year],
   eosr_p_low = eval_tb[which.max(profit_hat_low), profit_hat_low],
   eosr_p_high = eval_tb[which.max(profit_hat_high), profit_hat_high],
   usdsr_y = eval_tb[which.min(abs(s_rate - info_tb$usdsr)), yield_hat], 
   usdsr_p = eval_tb[which.min(abs(s_rate - info_tb$usdsr)), profit_hat_year],
   sqsr_y = eval_tb[which.min(abs(s_rate - info_tb$sqsr)), yield_hat], 
   sqsr_p = eval_tb[which.min(abs(s_rate - info_tb$sqsr)), profit_hat_year],
   ymsr = eval_tb[which.max(yield_hat), s_rate],
   ymsr_p = eval_tb[which.max(yield_hat), profit_hat_year],
   ymsr_y = eval_tb[which.max(yield_hat), yield_hat]
    )]

  # Write the updated info_tb back to file
  saveRDS(eval_tb, here("Data", "processed", "Analysis_results", paste0(ffy_id, "_eval_tb.rds")))
  saveRDS(info_tb, here("Data", "processed", "Analysis_results", paste0(ffy_id, "_info_tb.rds")))
  
  info_tb_list[[i]] <- info_tb
  best_gam_list[[i]] <- best_formula
}

 saveRDS(best_gam_list, here("Data", "Processed", "Analysis_results", "best_gam_list.rds"))
 saveRDS(info_tb_list, here("Data", "Processed", "Analysis_results", "info_tb_list.rds"))


```

```{r  after gam analysis data filtering , cache = T, results = "hide"}
 
# evaluate SQ_SR and USDA_SR with EOSR

# EOSR - SQ_SR and EOSR - USDA_SR
 
best_gam_list <-readRDS(here("Data", "Processed", "Analysis_results", "best_gam_list.rds"))
info_tb_list <-readRDS(here("Data", "Processed", "Analysis_results", "info_tb_list.rds"))

target_ffy_ids <- c("15_1_2023", "15_2_2023", "15_3_2023", "27_1_2023", "9_1_2022", "9_2_2022")

match_ind_remove <- which(sapply(info_tb_list, function(df) any(df$ffy_id %in% target_ffy_ids)))

# Identify elements to remove based on EOSR_y < 70 & is.na(USDSR)
cond_ind_remove <- which(sapply(info_tb_list, function(df) any(df$eosr_y < 70 | is.na(df$usdsr))))

# Combine both conditions
remove_indices <- unique(c(match_ind_remove , cond_ind_remove))

# Apply removal only if there are elements to remove
if (length(remove_indices) > 0) {
  best_gam_list <- best_gam_list[-remove_indices]
  info_tb_list <- info_tb_list[-remove_indices]
}

# Print remaining length to confirm : both have 93 
cat("Remaining elements in best_gam_list:", length(best_gam_list), "\n")
cat("Remaining elements in info_tb_list:", length(info_tb_list), "\n")

sq_wth_reg <-bind_rows(info_tb_list)

# Add price information tab to sq_wth_reg ( recent 5 year, previous year, current year)
# sq_wth_reg  and price_tab

setDT(sq_wth_reg)
setDT(price_tab)

# Step 1: Compute price ratio for each year
price_tab[, price_ratio_t := seed_price / corn_price]

# Step 2: Compute 5-year rolling average price ratio (excluding current year)
price_tab[, price_ratio5 := shift(frollmean(price_ratio_t, 5, align = "right"))]

# Step 3: Compute last year's price ratio
price_tab[, price_ratio1 := shift(frollmean(price_ratio_t, 1, align = "right"))]

 # Merge sq_wth_reg with price_tab on 'year'

 dat_result <- merge(sq_wth_reg, price_tab, by = "year", all.x = TRUE)

# Compute delta seeding rates
 dat_result[, delta_sqsr := sqsr - eosr]
 dat_result[, delta_usdsr := usdsr - eosr]
dat_result[, delta_p_sqsr := usdsr_p - eosr_p]
dat_result[, delta_p_usdsr := usdsr_p - eosr_p]

# Compute differences for precipitation, GDD, and price ratio
 dat_result[, delta_prcp := prcp_5 - prcp_30]
 dat_result[, delta_gdd := gdd_5 - gdd_30]
 dat_result[, delta_edd := edd_5 - edd_30]
 
 dat_result[, delta_price_ratio := price_ratio5 - price_ratio1]

# saveRDS(dat_result, here("Data", "Processed", "Analysis_results", "dat_result.rds"))


```

```{r sqsr and usdasr evaluation , cache = T, results = "hide"}

 dat_result <- readRDS(here("Data", "Processed", "Analysis_results", "dat_result.rds"))

# Hypothesis1. 

# Run quantile regression for delta_sqsr

# Define quantiles to estimate
quantiles <- c(0, 0.2, 0.4,0.6, 0.8, 1)

# Identify quantiles where delta_sqsr transitions from negative to positive
quantile_values <- quantile(dat_hypo$delta_sqsr, probs = c(0,0.2, 0.4, 0.6, 0.8, 1))

# Display quantile values and count observations per quantile
quantile_ranges <- cut(dat_hypo$delta_sqsr, breaks = quantile_values, include.lowest = TRUE)
table(quantile_ranges)

# Define relevant quantiles based on transition points

# Run quantile regression for Hypothesis 1
qr_h1 <- rq(delta_sqsr ~ delta_prcp + delta_gdd, tau = c(0.2, 0.4, 0.6, 0.8), data = dat_hypo)
summary(qr_h1)


### Hypothesis 2 ####

# Hypothesis2 
# Quantile regression to model how farmers’ over/under-seeding relates 
  # to yield variance structure(yield volatility) is related with delta_sqsr
# grouped the response (delta_sqsr) to check 
  # whether the predictors (pc1, pc2) behave differently across quantiles.
  # pc1 : overall variance level across seed rate quantiles.
  # pc2 : shape (curvature) of the variance curve.

# Loop through each row of dat_hypo
for (i in 1:nrow(dat_hypo)) {
    ffy_id_sel  <- dat_hypo$ffy_id[i]
    
    # Load eval_tb for the selected field
    eval_tb <- readRDS(here("Data", "processed", "Analysis_results", paste0(ffy_id_sel, "_eval_tb.rds")))
    eval_tb <- eval_tb[, .(s_rate, yield_hat)]
    
    # Assign seeding rate quantiles within each field
    eval_tb[, s_rate_quantile := cut(s_rate, 
                                     breaks = quantile(s_rate, probs = c(0,0.2,0.4,0.6,0.8,1), na.rm = TRUE), 
                                     include.lowest = TRUE)]
    
    # Compute yield variance within each quantile
    yield_variance_quantiles <- eval_tb[, .(var_y_q1 = var(yield_hat[s_rate_quantile == levels(s_rate_quantile)[1]], na.rm = TRUE),
                                            var_y_q2 = var(yield_hat[s_rate_quantile == levels(s_rate_quantile)[2]], na.rm = TRUE),
                                            var_y_q3 = var(yield_hat[s_rate_quantile == levels(s_rate_quantile)[3]], na.rm = TRUE),
                                            var_y_q4 = var(yield_hat[s_rate_quantile == levels(s_rate_quantile)[4]], na.rm = TRUE),
                                            var_y_q5 = var(yield_hat[s_rate_quantile == levels(s_rate_quantile)[5]], na.rm = TRUE))]
    
    # Update dat_hypo with yield variance quantiles for the current field
    dat_hypo[ffy_id == ffy_id_sel, `:=` (var_y_q1 = yield_variance_quantiles$var_y_q1,
                                          var_y_q2 = yield_variance_quantiles$var_y_q2,
                                          var_y_q3 = yield_variance_quantiles$var_y_q3,
                                          var_y_q4 = yield_variance_quantiles$var_y_q4,
                                          var_y_q5 = yield_variance_quantiles$var_y_q5)]
}

 library(pls)
 library(quantreg)

# Check correlation between variance variables
cor_matrix <- cor(dat_hypo[, .(var_y_q1, var_y_q2, var_y_q3, var_y_q4, var_y_q5)], use = "complete.obs")
print(cor_matrix)

# Apply PCA since the multicorrelation is relatively high
  
    pca_vars <- prcomp(dat_hypo[, .(var_y_q1, var_y_q2, var_y_q3, var_y_q4, var_y_q5)], scale. = TRUE)
    dat_hypo[, c("pc1", "pc2")] <- as.data.table(pca_vars$x[, 1:2])
    
    # Run regression using principal components
    # modeling delta_sqsr (farmer’s seeding deviation) as a function of
    # pc1: overall variance level across seed rate quantiles.
    # pc2: shape (curvature) of the variance curve.
    qr_h2 <- rq(delta_sqsr ~ pc1 + pc2, tau = c(0.2,0.4,0.6,0.8), data = dat_hypo)

 summary(qr_h2)

# Summarize regression results)
# diagnostic: check
   # - 1) stability of the regression estimates, 
   # - 2) multicollinearrity or weak signal at extrem tau values.

# Creates a new variable tau_group

dat_hypo[, tau_group := cut(delta_sqsr, breaks = quantile(delta_sqsr, probs = c(0, 0.25, 0.5, 0.75, 1), na.rm = TRUE))]

# Calculates the standard deviation of the principal components (pc1 and pc2)
#  within each delta_sqsr quartile group.
# Purpose: to check how much variability there is in the predictors
#  within each quantile of the response.
dat_hypo[, .(sd_pc1 = sd(pc1), sd_pc2 = sd(pc2)), by = tau_group]


# Run quantile regression for Hypothesis 3
qr_h3 <- rq(delta_sqsr ~ delta_price_ratio, tau = c(0.2, 0.4, 0.6, 0.8), data = dat_hypo)
summary(qr_h3)



```

## Visualization of Effects

```{r}



```